{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7: Angelie Kraft and Anton Wiehe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class BanditEnv:\n",
    "    def __init__(self, num_bandits=10):\n",
    "        # initialize list with num_bandit mean values\n",
    "        self.bandits = np.random.normal(loc=0, scale=1, size=num_bandits)\n",
    "        self.optimal_action = np.argmax(self.bandits) # determine max\n",
    "    \n",
    "    def step(self, action):\n",
    "        # return sample from distribution with idx == action\n",
    "        return np.random.normal(loc=self.bandits[action], scale=1, size=1)\n",
    "    \n",
    "    def is_optimal_action(self, action):\n",
    "        return action == self.optimal_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBanditAgent:\n",
    "    def __init__(self, epsilon, num_actions):\n",
    "        # initialize counts for actions and q values to 0\n",
    "        self.q_vals = np.zeros(num_actions)\n",
    "        self.counts = np.zeros(num_actions)\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def act(self):\n",
    "        # choose action/bandit with epsilon-greedy q-value\n",
    "        if np.random.random() > self.epsilon:\n",
    "            return np.argmax(self.q_vals)\n",
    "        else:\n",
    "            return np.random.randint(len(self.q_vals))\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        # update action based on reward\n",
    "        self.counts[action] += 1\n",
    "        self.q_vals[action] += [reward - self.q_vals[action]] / self.counts[action]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(epsilon, steps):\n",
    "    env = BanditEnv()\n",
    "    agent = SimpleBanditAgent(epsilon, len(env.bandits))\n",
    "    \n",
    "    rewards = np.empty(steps)\n",
    "    optimal_choices = np.empty(steps)\n",
    "    for i in range(steps):\n",
    "        # main algorithm\n",
    "        action = agent.act()\n",
    "        reward = env.step(action)\n",
    "        agent.update(action, reward)\n",
    "        \n",
    "        # track performance\n",
    "        rewards[i] = reward\n",
    "        optimal_choices[i] = env.is_optimal_action(action)\n",
    "        \n",
    "    return rewards, optimal_choices\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(epsilon, trials, steps):\n",
    "    exp_rewards = np.empty((trials, steps))\n",
    "    exp_optimal_choices = np.empty((trials, steps))\n",
    "    for i in range(trials):\n",
    "        rewards, optimal_choices = run(epsilon, steps)\n",
    "        exp_rewards[i] = rewards\n",
    "        exp_optimal_choices[i] = optimal_choices\n",
    "        \n",
    "    mean_rewards = np.mean(exp_rewards, axis=0)\n",
    "    mean_optimal_choices = np.mean(exp_optimal_choices, axis=0)\n",
    "    \n",
    "    #print(\"optimal choices: \", mean_optimal_choices)\n",
    "    \n",
    "    return mean_rewards, mean_optimal_choices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def meanSmoothing(x, N):\n",
    "    x = np.array(x)\n",
    "    out = np.zeros_like(x, dtype=np.float64)\n",
    "    dim_len = x.shape[0]\n",
    "    for i in range(dim_len):\n",
    "        if N % 2 == 0:\n",
    "            a, b = i - (N - 1) // 2, i + (N - 1) // 2 + 2\n",
    "        else:\n",
    "            a, b = i - (N - 1) // 2, i + (N - 1) // 2 + 1\n",
    "        # cap indices to min and max indices\n",
    "        a = max(0, a)\n",
    "        b = min(dim_len, b)\n",
    "        out[i] = np.mean(x[a:b])\n",
    "    return out\n",
    "\n",
    "\n",
    "def plot(exp_rewards, exp_choices, epsilons):\n",
    "    fig_rewards, ax_rewards = plt.subplots()\n",
    "    fig_choices, ax_choices = plt.subplots()\n",
    "    \n",
    "    for i in range(len(epsilons)):\n",
    "        label = \"epsilon = \" + str(epsilons[i])\n",
    "        ax_rewards.plot(meanSmoothing(exp_rewards[i], 15), label=label)\n",
    "        ax_choices.plot(meanSmoothing(exp_choices[i], 15), label=label)\n",
    "        \n",
    "    ax_rewards.set_xlabel('Step')\n",
    "    ax_rewards.set_ylabel('Mean reward per step')\n",
    "    ax_choices.set_xlabel('Step')\n",
    "    ax_choices.set_ylabel('Mean number of optimal action selection per step')\n",
    "    \n",
    "    fig_rewards.subplots_adjust(bottom=0.15)\n",
    "    fig_rewards.legend(loc=\"lower center\", ncol=3)\n",
    "    fig_choices.subplots_adjust(bottom=0.15)\n",
    "    fig_choices.legend(loc=\"lower center\", ncol=3)\n",
    "    \n",
    "    \n",
    "    fig_rewards.savefig('fig_rewards.pdf')\n",
    "    fig_choices.savefig('fig_choices.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start(trials=50, steps=1000):\n",
    "    epsilons = [0.1, 0.01, 0.009]\n",
    "    exp_rewards = np.empty((len(epsilons), steps))\n",
    "    exp_choices = np.empty((len(epsilons), steps))\n",
    "    for i in range(len(epsilons)):\n",
    "        exp_rewards[i], exp_choices[i] = run_experiment(epsilons[i], trials, steps)\n",
    "    plot(exp_rewards, exp_choices, epsilons)\n",
    "    return exp_rewards, exp_choices\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
